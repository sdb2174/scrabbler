{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The goal of this is to explore the current setup and find an efficient way to run simulations during our search process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate Main\n",
    "We want to start up the board as main does, but run multiple plays rather than just doing one and terminating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import scrabbler as sc\n",
    "import random "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning with value function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scrabble_environment import ScrabbleEnvironment\n",
    "\n",
    "# Define hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000  # Number of training episodes\n",
    "batch_size = 32  # Batch size for experience replay\n",
    "num_hidden_units = 32  # Number of hidden units in the neural network\n",
    "\n",
    "# Define the function approximator\n",
    "inputs = tf.keras.Input(shape=(15, 15, 27), name='board')  # Input layer\n",
    "flatten = tf.keras.layers.Flatten()(inputs)  # Flatten the input\n",
    "hidden = tf.keras.layers.Dense(num_hidden_units, activation='relu')(flatten)  # Hidden layer\n",
    "outputs = tf.keras.layers.Dense(7)(hidden)  # Output layer\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define the Scrabble environment\n",
    "env = ScrabbleEnvironment()\n",
    "\n",
    "# Define the replay buffer\n",
    "replay_buffer = []\n",
    "\n",
    "# Main loop for training\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Choose an action\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 6)\n",
    "        else:\n",
    "            q_values = model.predict(np.array([state]))\n",
    "            action = np.argmax(q_values[0])\n",
    "        \n",
    "        # Take the action and observe the next state and reward\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Add the experience to the replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Update the Q-values using experience replay\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # Sample a batch of experiences from the replay buffer\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            \n",
    "            # Compute the target Q-values\n",
    "            targets = []\n",
    "            states = []\n",
    "            for experience in batch:\n",
    "                state, action, reward, next_state, done = experience\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    q_values = model.predict(np.array([next_state]))\n",
    "                    target = reward + gamma * np.max(q_values[0])\n",
    "                targets.append(target)\n",
    "                states.append(state)\n",
    "            targets = np.array(targets)\n",
    "            states = np.array(states)\n",
    "            \n",
    "            # Update the Q-values using gradient descent\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(states, training=True)\n",
    "                actions_one_hot = tf.one_hot(batch[:, 1], depth=7)\n",
    "                q_values = tf.reduce_sum(actions_one_hot * q_values, axis=1)\n",
    "                loss = loss_fn(targets, q_values)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            \n",
    "    # Decay the exploration rate\n",
    "    epsilon = max(0.1, epsilon * 0.99)\n",
    "    \n",
    "    # Print the total reward for the episode\n",
    "    print(f'Episode {episode}: Total reward = {total_reward}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other\n",
    "Vectorize the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters_left =['V','N','A','A'] # Example leave\n",
    "\n",
    "\n",
    "def vectorizedLetterCombos(letters):\n",
    "    vectorized = [0]*26 # Our vectorized version of the leaves which we will update to represent the leaves below\n",
    "    for letter in letters:\n",
    "        index = ord(letter) - 65\n",
    "        vectorized[index] += 1\n",
    "    return vectorized\n",
    "vectorizedLetterCombos(letters_left)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's vectorized what hasn't been seen yet, ie what's left in the bag + the person's hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 2, 6, 3, 2, 1, 6, 4, 4, 1, 2, 2, 1, 2, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BAG example:\n",
    "\n",
    "#We will initialize this at the start of the game, and as the game progress, update this list\n",
    "unseen = [\n",
    "    \"A\",\"A\",\n",
    "    \"B\",\n",
    "    \"K\",\n",
    "    \"L\",\"L\",\"L\",\"L\",\n",
    "    \"M\", \"M\", \n",
    "    \"N\",\"N\",\"N\",\"N\",\"N\",\"N\",\n",
    "    \"O\",\"O\",\"O\",\n",
    "    \"P\", \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\"R\",\"R\",\"R\",\"R\",\"R\",\n",
    "    \"S\",\"S\",\"S\",\"S\",\n",
    "    \"T\",\"T\",\"T\",\"T\",\n",
    "    \"U\",\n",
    "    \"V\", \"V\",\n",
    "    \"W\", \"W\",\n",
    "    \"X\",\n",
    "    \"Y\", \"Y\"\n",
    "    ]\n",
    "player_rack = [\"A\",\"C\",\"D\",\"A\",\"E\",\"M\",\"N\"]\n",
    "\n",
    "vec_bag = vectorizedLetterCombos(unseen) + vectorizedLetterCombos(player_rack) # the vectorized version of unseen tiles\n",
    "vec_bag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
